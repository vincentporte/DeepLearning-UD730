{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UDACITYÂ UD730 - assignement 2 fully connected network\n",
    "#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb\n",
    "#The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings.\n",
    "\"\"\"\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Let's load all the data into TensorFlow and build the computation graph corresponding to our training:\n",
    "\"\"\"\n",
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.356243\n",
      "Training accuracy: 8.1%\n",
      "Validation accuracy: 10.1%\n",
      "Loss at step 100: 2.230855\n",
      "Training accuracy: 72.5%\n",
      "Validation accuracy: 70.4%\n",
      "Loss at step 200: 1.803812\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 72.9%\n",
      "Loss at step 300: 1.570590\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 73.7%\n",
      "Loss at step 400: 1.409979\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 500: 1.291038\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 600: 1.198794\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 700: 1.124343\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 800: 1.062301\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 74.9%\n",
      "Test accuracy: 82.9%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Let's run this computation and iterate\n",
    "\"\"\"\n",
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, \n",
    "we create a Placeholder node which will be fed actual data at every call of session.run().\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 18.035139\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 500: 1.897530\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 1000: 0.873708\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1500: 1.149111\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 2000: 1.272744\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 2500: 0.823051\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 3000: 1.034623\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Problem\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() \n",
    "and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "batch size = 128\n",
    "learning rate = 0.5   | num steps = 3001  | test accuracy = 88,5%\n",
    "learning rate = 0.05  | num steps = 3001  | test accuracy = 88,6%\n",
    "learning rate = 0.05  | num steps = 30001 | test accuracy = 91,7%\n",
    "learning rate = 0.005 | num steps = 30001 | test accuracy = 89,7%\n",
    "learning rate = 0.005 | num steps = 60001 | test accuracy = 90,7%\n",
    "\n",
    "batch size = 256 | learning rate = 0.05   | num steps = 30001 | test accuracy = 91,6%\n",
    "batch size = 512 | learning rate = 0.05   | num steps = 30001 | test accuracy = 91,7%\n",
    "batch size = 512 | learning rate = 0.005  | num steps = 30001 | test accuracy = 86,6%\n",
    "batch size = 512 | learning rate = 0.005  | num steps = 60001 | test accuracy = 90,4%\n",
    "\n",
    "Intuitively, how does mini-batch size affect the performance of (stochastic) gradient descent? \n",
    "https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "num_steps = 30001 #training cycle | training_epochs\n",
    "batch_size = 512\n",
    "#display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 1024 # 1st layer number of features\n",
    "n_input = image_size * image_size # data input (img shape: 28*28)\n",
    "#num_labels = 10 #(0-9)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def oneHiddenLayerModel (x, weights, biases):\n",
    "  # Hidden layer with RELU activation\n",
    "  hiddenLayer = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "  hiddenLayer = tf.nn.relu(hiddenLayer)\n",
    "  # Output layer with linear activation\n",
    "  outputLayer = tf.add(tf.matmul(hiddenLayer, weights['out']), biases['out'])\n",
    "  return outputLayer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n_input))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables : layers weight & bias\n",
    "  weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_1, num_labels]))\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    " \n",
    "  # Construct model\n",
    "  pred = oneHiddenLayerModel(tf_train_dataset, weights, biases)\n",
    "\n",
    "  # Loss (Cost)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(pred)\n",
    "  valid_prediction = tf.nn.softmax(oneHiddenLayerModel(tf_valid_dataset, weights, biases))\n",
    "  test_prediction = tf.nn.softmax(oneHiddenLayerModel(tf_test_dataset, weights, biases))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 329.959\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 15.3%\n",
      "Minibatch loss at step 5000 : 6.43911\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 10000 : 2.94505\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 15000 : 1.69728\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 20000 : 1.76259\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 25000 : 1.66547\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 30000 : 1.43185\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print ('Initialized')\n",
    "  summary_data = np.zeros((num_steps, 3))\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    valid_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "    summary_data[step] = [step, valid_accuracy, l]\n",
    "    \n",
    "    if (step % 5000 == 0):\n",
    "      print (\"Minibatch loss at step\", step, \":\", l)\n",
    "      print (\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print (\"Validation accuracy: %.1f%%\" % valid_accuracy)\n",
    "  print (\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30001\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x216922729e8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAADFCAYAAABNRWsIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrEM4UzczM+sQ/x919b65WQFG2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216911f0be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to go further, plot accuracies (train & validation) and loss through epochs regarding range of parameters\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print (summary_data.shape[0])\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.plot(summary_data[:,0], summary_data[:,1], 'b-')\n",
    "ax1.set_xlabel(\"Time Step\")\n",
    "ax1.set_ylabel(\"Accuracy of Validation Set\")\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.plot(summary_data[:,0], summary_data[:,2], 'b-')\n",
    "ax2.set_xlabel(\"Time Step\")\n",
    "ax2.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
