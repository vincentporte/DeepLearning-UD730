{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datas loaded [debug = 0]\n",
      "Reshaped Training set   (1000000, 784) (1000000, 10)\n",
      "    300001 |        1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             0.50 |         \n",
      "Initialized\n",
      "Minibatch loss at step 0 : 9945.208 - acc: 9.7% | Validation acc: 21.5%\n",
      "Minibatch loss at step 5000 : 315.654 - acc: 20.2% | Validation acc: 22.8%\n",
      "Minibatch loss at step 10000 : 194.595 - acc: 34.8% | Validation acc: 40.2%\n",
      "Minibatch loss at step 15000 : 121.136 - acc: 54.9% | Validation acc: 65.3%\n",
      "Minibatch loss at step 20000 : 76.268 - acc: 63.6% | Validation acc: 75.3%\n",
      "Minibatch loss at step 25000 : 48.538 - acc: 71.5% | Validation acc: 79.4%\n",
      "Minibatch loss at step 30000 : 31.236 - acc: 74.6% | Validation acc: 81.6%\n",
      "Minibatch loss at step 35000 : 20.402 - acc: 77.7% | Validation acc: 82.8%\n",
      "Minibatch loss at step 40000 : 13.474 - acc: 79.5% | Validation acc: 83.5%\n",
      "Minibatch loss at step 45000 : 8.991 - acc: 82.5% | Validation acc: 84.2%\n",
      "Minibatch loss at step 50000 : 6.186 - acc: 81.3% | Validation acc: 84.7%\n",
      "Minibatch loss at step 55000 : 4.292 - acc: 83.9% | Validation acc: 85.2%\n",
      "Minibatch loss at step 60000 : 3.050 - acc: 84.8% | Validation acc: 85.8%\n",
      "Minibatch loss at step 65000 : 2.209 - acc: 86.3% | Validation acc: 86.6%\n",
      "Minibatch loss at step 70000 : 1.669 - acc: 85.3% | Validation acc: 87.0%\n",
      "Minibatch loss at step 75000 : 1.356 - acc: 84.8% | Validation acc: 87.4%\n",
      "Minibatch loss at step 80000 : 1.110 - acc: 84.4% | Validation acc: 87.8%\n",
      "Minibatch loss at step 85000 : 0.894 - acc: 87.4% | Validation acc: 88.1%\n",
      "Minibatch loss at step 90000 : 0.775 - acc: 87.0% | Validation acc: 88.3%\n",
      "Minibatch loss at step 95000 : 0.724 - acc: 85.9% | Validation acc: 88.4%\n",
      "Minibatch loss at step 100000 : 0.662 - acc: 86.2% | Validation acc: 88.5%\n",
      "Minibatch loss at step 105000 : 0.539 - acc: 89.6% | Validation acc: 89.1%\n",
      "Minibatch loss at step 110000 : 0.517 - acc: 89.0% | Validation acc: 89.2%\n",
      "Minibatch loss at step 115000 : 0.526 - acc: 88.8% | Validation acc: 89.4%\n",
      "Minibatch loss at step 120000 : 0.489 - acc: 89.3% | Validation acc: 89.5%\n",
      "Minibatch loss at step 125000 : 0.470 - acc: 89.3% | Validation acc: 88.9%\n",
      "Minibatch loss at step 130000 : 0.474 - acc: 89.3% | Validation acc: 89.0%\n",
      "Minibatch loss at step 135000 : 0.504 - acc: 88.1% | Validation acc: 89.1%\n",
      "Minibatch loss at step 140000 : 0.516 - acc: 87.4% | Validation acc: 89.1%\n",
      "Minibatch loss at step 145000 : 0.453 - acc: 89.8% | Validation acc: 89.6%\n",
      "Minibatch loss at step 150000 : 0.448 - acc: 89.7% | Validation acc: 89.5%\n",
      "Minibatch loss at step 155000 : 0.507 - acc: 87.9% | Validation acc: 89.6%\n",
      "Minibatch loss at step 160000 : 0.480 - acc: 87.9% | Validation acc: 89.6%\n",
      "Minibatch loss at step 165000 : 0.488 - acc: 88.4% | Validation acc: 89.9%\n",
      "Minibatch loss at step 170000 : 0.464 - acc: 88.9% | Validation acc: 89.9%\n",
      "Minibatch loss at step 175000 : 0.475 - acc: 88.1% | Validation acc: 90.0%\n",
      "Minibatch loss at step 180000 : 0.494 - acc: 88.0% | Validation acc: 90.0%\n",
      "Minibatch loss at step 185000 : 0.488 - acc: 89.1% | Validation acc: 89.9%\n",
      "Minibatch loss at step 190000 : 0.468 - acc: 89.3% | Validation acc: 90.0%\n",
      "Minibatch loss at step 195000 : 0.468 - acc: 88.4% | Validation acc: 90.0%\n",
      "Minibatch loss at step 200000 : 0.474 - acc: 89.0% | Validation acc: 90.0%\n",
      "Minibatch loss at step 205000 : 0.420 - acc: 90.7% | Validation acc: 90.3%\n",
      "Minibatch loss at step 210000 : 0.420 - acc: 90.8% | Validation acc: 90.3%\n",
      "Minibatch loss at step 215000 : 0.457 - acc: 89.1% | Validation acc: 90.4%\n",
      "Minibatch loss at step 220000 : 0.445 - acc: 90.4% | Validation acc: 90.4%\n",
      "Minibatch loss at step 225000 : 0.491 - acc: 89.1% | Validation acc: 89.8%\n",
      "Minibatch loss at step 230000 : 0.459 - acc: 89.7% | Validation acc: 89.7%\n",
      "Minibatch loss at step 235000 : 0.483 - acc: 88.4% | Validation acc: 89.8%\n",
      "Minibatch loss at step 240000 : 0.453 - acc: 89.8% | Validation acc: 89.8%\n",
      "Minibatch loss at step 245000 : 0.462 - acc: 89.6% | Validation acc: 89.9%\n",
      "Minibatch loss at step 250000 : 0.424 - acc: 90.3% | Validation acc: 89.9%\n",
      "Minibatch loss at step 255000 : 0.407 - acc: 90.6% | Validation acc: 89.9%\n",
      "Minibatch loss at step 260000 : 0.447 - acc: 89.1% | Validation acc: 89.9%\n",
      "Minibatch loss at step 265000 : 0.483 - acc: 88.6% | Validation acc: 90.2%\n",
      "Minibatch loss at step 270000 : 0.413 - acc: 90.3% | Validation acc: 90.2%\n",
      "Minibatch loss at step 275000 : 0.439 - acc: 89.8% | Validation acc: 90.3%\n",
      "Minibatch loss at step 280000 : 0.417 - acc: 91.1% | Validation acc: 90.2%\n",
      "Minibatch loss at step 285000 : 0.464 - acc: 89.3% | Validation acc: 90.3%\n",
      "Minibatch loss at step 290000 : 0.431 - acc: 90.7% | Validation acc: 90.2%\n",
      "Minibatch loss at step 295000 : 0.479 - acc: 88.4% | Validation acc: 90.2%\n",
      "Minibatch loss at step 300000 : 0.440 - acc: 90.3% | Validation acc: 90.2%\n",
      "Test accuracy: 95.7%\n",
      "Model saved in file: ./model.20170425232152.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAADFCAYAAAC4u+C4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3jAMwzCKEDPwhmEYhlGE/H+hqHan7k50VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24b8120cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Learning\n",
    "Assignment 3 - problem 4\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "The goal of this assignment is to explore regularization techniques.\n",
    "\"\"\"\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "\"\"\"\n",
    "Run SGD with a hidden layer and regularization.\n",
    "> num_steps:\n",
    "     Number of steps to run SGD minibatch training. In each step a new minibatch is fed through the network.\n",
    "> learning_rate:\n",
    "     SGD learning rate, passed direction to GradientDescentOptimizer.\n",
    "> l2_reg_beta:\n",
    "     The loss is regularized with L2 loss on all weights. This is the regularization coefficient. \n",
    "     Setting it to 0 means no L2 loss regularization. Otherwise, small values work well.\n",
    "> dropout_keep_prob:\n",
    "     Dropout \"keep\" probability for outputs of the hidden layer. Set to 1 to avoid dropout altogether.\n",
    "\"\"\"\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    #Reformat into a shape that's more adapted to the models we're going to train:\n",
    "    #data as a flat matrix, labels as float 1-hot encodings.\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(n_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def hiddenLayersModel (x, w, b, keep_prob):\n",
    "    #Dropout \"keep\" probability for outputs of the hidden layer.\n",
    "    #Set to 1 to avoid dropout altogether, ie for validation and test pred  \n",
    "\n",
    "    # Hidden layer #1 with RELU activation + dropout\n",
    "    hiddenLayer1 = tf.add(tf.matmul(x, w['h1']), b['b1'])\n",
    "    hiddenLayer1 = tf.nn.relu(hiddenLayer1)\n",
    "    hiddenLayer1 = tf.nn.dropout(hiddenLayer1,keep_prob=keep_prob)\n",
    "    # Hidden layer #2 with RELU activation + dropout\n",
    "    hiddenLayer2 = tf.add(tf.matmul(hiddenLayer1, w['h2']), b['b2'])\n",
    "    hiddenLayer2 = tf.nn.relu(hiddenLayer2)\n",
    "    hiddenLayer2 = tf.nn.dropout(hiddenLayer2,keep_prob=keep_prob)\n",
    "    # Output layer with linear activation\n",
    "    outputLayer = tf.add(tf.matmul(hiddenLayer2, w['out']), b['out'])\n",
    "    return outputLayer\n",
    "\n",
    "def run_SGD_hiddenLayers_Reg_Dropout(graph, num_steps=3001, learning_rate=0.1, \n",
    "                                     l2_reg_beta=0.0, dropout_keep_prob=1, \n",
    "                                     decay_steps=1000, decay_rate=0.96):\n",
    "\n",
    "    print('    %d |        %d |       %0.5f |       %d |       %0.2f |     %0.5f |             %0.2f |         '\n",
    "         % (num_steps, batch_size, learning_rate, decay_steps, decay_rate, l2_reg_beta, dropout_keep_prob))\n",
    "\n",
    "    with graph.as_default():\n",
    "                       \n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n_input))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, n_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "        # Variables : layers weight & bias\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1])),\n",
    "            'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "            'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_labels]))\n",
    "        }\n",
    "        \n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "            'out': tf.Variable(tf.zeros([n_labels]))\n",
    "        }\n",
    " \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Construct model\n",
    "        pred = hiddenLayersModel(tf_train_dataset, weights, biases, dropout_keep_prob)\n",
    "\n",
    "        # Loss (Cost)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf_train_labels))\n",
    "        reg_loss = tf.reduce_mean(loss + l2_reg_beta * (tf.nn.l2_loss(weights['h1']) \n",
    "                                                        + tf.nn.l2_loss(weights['h2'])\n",
    "                                                        + tf.nn.l2_loss(weights['out'])\n",
    "                                                       )\n",
    "                                 )\n",
    "\n",
    "        # Optimizer.\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(reg_loss)\n",
    "        global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=learning_rate, global_step=global_step,\n",
    "                                                   decay_steps=decay_steps,decay_rate=decay_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(reg_loss, global_step=global_step)\n",
    "\n",
    "  \n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(pred)\n",
    "        valid_prediction = tf.nn.softmax(hiddenLayersModel(tf_valid_dataset, weights, biases, 1)) #no dropout\n",
    "        test_prediction = tf.nn.softmax(hiddenLayersModel(tf_test_dataset, weights, biases, 1)) #no dropout\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print ('Initialized')\n",
    "        summary_data = np.zeros((num_steps, 3))\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, reg_loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            if (step % 5000 == 0):\n",
    "                valid_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "                if step == 0:\n",
    "                    index = 0\n",
    "                else:\n",
    "                    index = int(step / 1000)\n",
    "                summary_data[index] = [step, valid_accuracy, l]\n",
    "                print (\"Minibatch loss at step %d : %.3f - acc: %.1f%% | Validation acc: %.1f%%\" \n",
    "                       % (step, l, accuracy(predictions, batch_labels),valid_accuracy))\n",
    "    \n",
    "        acc = accuracy(test_prediction.eval(), test_labels)\n",
    "        print (\"Test accuracy: %.1f%%\" % acc)\n",
    "        #print('    %d |        %d |       %0.5f |       %d |       %0.2f |     %0.5f |             %0.2f |         %.1f%%'\n",
    "        #      % (num_steps, batch_size, learning_rate, decay_steps, decay_rate, l2_reg_beta, dropout_keep_prob, acc))\n",
    "\n",
    "        save_path = saver.save(session, \"./model.%s.ckpt\" % '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now()))\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        return summary_data\n",
    "\n",
    "def plotting(data):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.plot(data[:,0], data[:,1], 'b-')\n",
    "    ax1.set_xlabel(\"Time Step\")\n",
    "    ax1.set_ylabel(\"Accuracy of Validation Set\")\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.plot(data[:,0], data[:,2], 'b-')\n",
    "    ax2.set_xlabel(\"Time Step\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "        \n",
    "def nudge_dataset(X,Y):\n",
    "\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    shift = lambda x, w: convolve(x.reshape((28, 28)), mode='constant',\n",
    "                                  weights=w).ravel()\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y\n",
    "    \n",
    "     \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    debug = 0\n",
    "    nudging_training_dataset = 1\n",
    "\n",
    "    #parameters\n",
    "    pickle_file = 'notMNIST.pickle'\n",
    "    image_size = 28\n",
    "    batch_size = 1408\n",
    "    n_hidden_1 = 1024 # 1st layer number of features\n",
    "    n_hidden_2 = 512 # 2st layer number of features\n",
    "    n_input = image_size * image_size # data input (img shape: 28*28)\n",
    "    n_labels = 10 #(0-9)\n",
    "    \n",
    "    #First reload the data we generated in 1_notmnist.ipynb\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        train_dataset = save['train_dataset']\n",
    "        train_labels = save['train_labels']\n",
    "        valid_dataset = save['valid_dataset']\n",
    "        valid_labels = save['valid_labels']\n",
    "        test_dataset = save['test_dataset']\n",
    "        test_labels = save['test_labels']\n",
    "        del save  # hint to help gc free up memory\n",
    "        print('datas loaded [debug = %d]' % debug)\n",
    "        train_dataset_o, train_labels_o = reformat(train_dataset, train_labels)\n",
    "        valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "        test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "        \n",
    "        if nudging_training_dataset == 1:\n",
    "            # nudge to get 5X training set\n",
    "            train_dataset, train_labels = nudge_dataset(train_dataset_o, train_labels_o)\n",
    "            # nudge Xn and Yn from last step to get 25X training set\n",
    "            #train_dataset, train_labels = nudge_dataset(Xn, Yn)\n",
    "        else:\n",
    "            train_dataset, train_labels = train_dataset_o, train_labels_o\n",
    "\n",
    "        print('Reshaped Training set  ', train_dataset.shape, train_labels.shape)\n",
    "        \n",
    "        if debug == 1:\n",
    "            print('Reshaped Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "            print('Reshaped Test set      ', test_dataset.shape, test_labels.shape)\n",
    "            num_steps=5\n",
    "        else:\n",
    "            num_steps=300001            \n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    summary_data = run_SGD_hiddenLayers_Reg_Dropout(graph,\n",
    "                                                    num_steps,\n",
    "                                                    learning_rate=0.05,\n",
    "                                                    l2_reg_beta=0.001,\n",
    "                                                    dropout_keep_prob=0.5,\n",
    "                                                    decay_steps=10000,\n",
    "                                                    decay_rate=0.96\n",
    "                                                   )\n",
    "    plotting(summary_data)\n",
    "\n",
    "\n",
    "\n",
    "#enlarge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "running SGD: 2 hidden layers : 1024 nodes / 512 nodes\n",
    "num steps | batch size | learning rate | decay steps | decay rate | l2 reg beta |dropout keep prob | test accuracy\n",
    "    40001 |        512 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         89.5%\n",
    "    40001 |        256 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         89.3%\n",
    "    60001 |        128 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         90.8%\n",
    "    60001 |        256 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         91.6%\n",
    "    60001 |        512 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         91.8%\n",
    "    60001 |       1024 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         91.7%\n",
    "    60001 |       1152 |       0.05000 |       10000 |       0.90 |     0.00100 |             0.50 |         91.6%\n",
    "    60001 |       1152 |       0.05000 |       10000 |       0.96 |     0.00100 |             1.00 |         94.6%    \n",
    "    60001 |       1280 |       0.05000 |       10000 |       0.96 |     0.00100 |             1.00 |         95.0% \n",
    "    60001 |       1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             1.00 |         95.3%***\n",
    "    60001 |       1536 |       0.05000 |       10000 |       0.96 |     0.00100 |             1.00 |         95.1%\n",
    "    60001 |       1408 |       0.05000 |        5000 |       0.99 |     0.00100 |             1.00 |         95.1%\n",
    "    60001 |       1408 |       0.05000 |        5000 |       0.98 |     0.00100 |             1.00 |         95.1%\n",
    "    60001 |       1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             0.90 |         94.8%\n",
    "    60001 |       1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             0.50 |         93.0%\n",
    "Reshaped Training set   (1000000, 784) (1000000, 10)\n",
    "    60001 |       1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             0.50 |         92.4%\n",
    "   300001 |       1408 |       0.05000 |       10000 |       0.96 |     0.00100 |             0.50 |         95.7%\n",
    "   maximum reached on validation set betweem 210'000 and 220'000' steps (90.4%)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
